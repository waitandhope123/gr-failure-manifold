# Phase 10 — Semiclassical & Quantum Fields

**Toys 061–070**

---

## Toy 061 — State-dependent semiclassical stress-energy near a Schwarzschild horizon

This toy computes a semiclassical stress–energy tensor for a massless conformal field on a fixed Schwarzschild background, reduced to the 1+1D $(t,r^\*)$ sector. The geometry is held fixed while the expectation value $\langle T_{ab}\rangle$ is evaluated using the Polyakov conformal anomaly in null coordinates $u=t-r^\*$ and $v=t+r^\*$, with metric $ds^2=-f(r)\,du\,dv$ and $f(r)=1-2M/r$. The quantities of interest are the null components $T_{uu}$, $T_{vv}$, the mixed component $T_{uv}$, and simple observer-based proxies such as the static energy density $\rho_{\text{static}}=T_{tt}/f$, which highlight how stress–energy behaves as the horizon $r\to2M$ is approached.

The toy exists to illustrate a conceptual limitation of semiclassical gravity: the spacetime metric alone does not determine the stress–energy tensor once quantum effects are included. Different quantum states correspond to different integration constants added to the anomaly term, so the same classical geometry admits inequivalent semiclassical behaviors. In this model the stress tensor takes the schematic form $T_{uu}=-(1/48\pi)[(\rho')^2-\rho'']+t_{uu}$ (and similarly for $T_{vv}$), and changing the constants $t_{uu},t_{vv}$ selects Boulware, Unruh, or Hartle–Hawking–like states. The toy is therefore a stress test: it exposes how horizon regularity or divergence is a state-dependent feature rather than a prediction of the classical solution itself.

The JSON output should be read comparatively across states and radii rather than as an absolute prediction. Near the horizon probe radius, the key diagnostic is whether $T_{uu}$ tends toward zero (regular future horizon) or remains finite, while away from the horizon the sign and magnitude of $\rho_{\text{static}}$ indicate how large the stress appears to a static observer. Trends with decreasing $r$ matter more than precise numbers, and cancellations between the anomaly term and the state-dependent constants are the central signal. The results should not be over-interpreted as full 4D physics or backreaction effects; they are a 2D proxy whose numerical values simply reflect how the algebraic structure of the Polyakov stress tensor maps onto the sampled geometry and the defining equations of the chosen quantum state.

---

## Toy 062 — Stochastic semiclassical backreaction in FLRW

This toy illustrates how adding irreducible stress–energy fluctuations alters the expansion history of a simple cosmological model. The physical setup is a flat Friedmann–Lemaître–Robertson–Walker spacetime filled with a perfect fluid of constant equation of state $p=w\rho$, whose classical solution has a smooth, deterministic Hubble rate. The quantity of interest is the Hubble parameter $H(t)$, modified by a stochastic source so that the effective evolution obeys the proxy relation $H^2(t)=\bar H^2(t)+\xi(t)$, where $\bar H(t)$ is the classical background and $\xi(t)$ represents fluctuating semiclassical stress–energy.

The toy exists to stress-test a common intuition in semiclassical gravity: that quantum effects can be treated purely as small corrections to mean quantities like $\langle T_{ab}\rangle$. By injecting noise whose variance scales with curvature, $\mathrm{Var}[\xi]=\alpha \bar H^4$, the model exposes a failure mode in which predictability degrades even when the mean curvature remains finite and well behaved. This is not a physical prediction of the early universe, but a controlled illustration that the “breakdown of GR” may thicken from a sharp boundary into a probabilistic region once fluctuations are taken seriously.

The JSON output should be read as an ensemble of stochastic realizations at fixed times. For each time $t$, individual samples report the background value $\bar H$, a sampled $H$, their difference $\Delta H$, and simple curvature proxies that scale like powers of $H$. The ensemble summaries aggregate these into a mean and variance of $H$, with the relative standard deviation indicating how large fluctuations are compared to the mean. What matters are trends—larger relative fluctuations at earlier times where $\bar H$ is larger—not the precise numerical values or any single realization. The curvature proxies should not be over-interpreted as exact invariants; they merely reflect the $H^2$ and $H^4$ scaling implied by the defining equations and connect the noisy expansion histories back to the underlying stochastic Friedmann relation.

---

## Toy 063 — Symmetry Breaking Without Singularity in Anisotropic Expansion

This toy illustrates the evolution of a spatially homogeneous but anisotropic universe of Bianchi I type, where the three orthogonal directions expand at different rates. Instead of a single scale factor, the geometry is characterized by directional Hubble rates $H_i$ and a mean expansion rate $H$, with anisotropy encoded in the shear $\sigma^2$. The quantity of interest is how this shear contributes to the overall dynamics through the generalized Friedmann constraint $3H^2 = 8\pi\rho + \sigma^2$, showing that even when matter density $\rho$ is small and regular, directional expansion can remain dynamically relevant.

The toy exists to expose how strongly standard cosmological intuition relies on exact symmetry assumptions. In isotropic FLRW models the shear vanishes identically, but here it decays only as a power of the mean scale factor and need not disappear quickly. This setup is a stress test for the idea that “generic” expanding universes automatically isotropize: the outcome depends on the equation of state $w$ and the initial shear parameters, not on any singular behavior or exotic matter. The model demonstrates a limitation of symmetry-reduced reasoning rather than making a physical prediction about the real universe.

The JSON output should be read as a time series of diagnostic quantities rather than precise observables. Trends in the ratio $\sigma/H$ indicate whether anisotropy becomes negligible relative to expansion, while the directional Hubble rates show how symmetry breaking manifests even when curvature scalars like the Ricci scalar $R=8\pi\rho(1-3w)$ steadily decrease. The Kretschmann scalar tracks overall curvature magnitude but should not be over-interpreted as signaling pathology or singular behavior. Small differences between the base run and the perturbed initial shear highlight sensitivity to initial conditions, connecting the reported values back to how shear enters the constraint and curvature expressions without implying new physics beyond the toy model.

---

## Toy 064 — Inhomogeneous Breakdown Without Curvature Singularity

This toy evolves a real scalar field on a fixed, weakly curved 1+1-dimensional spacetime to illustrate how nonlinear matter dynamics can fail even when geometry remains completely regular. The background metric is static and everywhere smooth, with line element $ds^2 = -f(x) dt^2 + f(x)^{-1} dx^2$, where $f(x)=1+\epsilon x^2$ sets a mild curvature scale, and the quantity of interest is the scalar field profile $\phi(t,x)$ and especially its spatial gradients. The field obeys a nonlinear wave equation, $\Box \phi + \lambda \phi^3 = 0$, so initially smooth, sinusoidal data are allowed to steepen as they propagate across the curved background.

The purpose of the toy is not to predict a physical instability, but to expose a specific limitation of classical field evolution: predictability can be lost through matter dynamics alone, without any curvature singularity or horizon formation. In this setup the spacetime curvature, measured for example by the Ricci scalar $R=-4\epsilon$, is finite and constant everywhere, yet nonlinear self-interaction can drive gradients of $\phi$ toward a shock-like divergence. The toy therefore functions as a stress test of the assumption that geometric regularity guarantees well-posed evolution, highlighting a failure mode where the equations remain formally valid but no longer yield controlled solutions.

The JSON output should be read as a time-ordered diagnostic record rather than a trajectory of physical observables. Each sample point reports the maximum field amplitude and the maximum spatial gradient at that time, alongside the fixed background curvature, allowing one to see whether $|\phi|$ stays bounded while $|\partial_x \phi|$ grows, oscillates, or crosses a preset blow-up threshold. The most important trend is whether the gradient diagnostic increases toward the threshold and whether a finite shock time is recorded; the absolute numerical values depend on resolution and thresholds and should not be over-interpreted as physical scales. The curvature entries are included to emphasize that any detected breakdown arises from the nonlinear term in the wave equation rather than from changes in the geometric quantities appearing in $\Box \phi$.

---

## Toy 065 — Geometry saturation without resolution: quasilocal energy ambiguity

This toy evaluates two standard notions of “energy contained inside a sphere” in an exactly specified Schwarzschild spacetime, using coordinate 2-spheres of areal radius $r$ around a black hole of mass $M$ (with $G=c=1$). The geometry is fixed by $M$, and the toy compares the Misner–Sharp mass (which for Schwarzschild is simply constant) to the static-slice, reference-subtracted Brown–York quasilocal energy, $E_{\mathrm{BY}}(r)=r\left(1-\sqrt{1-2M/r}\right)$, reported only where this expression is real-valued. Alongside these energies it reports simple curvature diagnostics (e.g., the vacuum condition $R=0$ and the Kretschmann scalar) to emphasize that nothing about the spacetime itself is underdetermined in this setup.

The purpose is to illustrate that “quasilocal energy” is not a single geometric invariant, even when the spacetime geometry is completely determined: different defensible definitions can assign different energies to the same 2-sphere. The pressure point is conceptual rather than predictive: the toy stress-tests the idea that specifying the metric (and even its invariants) is enough to fix a unique notion of enclosed energy, by exposing a built-in ambiguity between $M_{\mathrm{MS}}(r)=M$ and the radius-dependent $E_{\mathrm{BY}}(r)$. Any disagreement here should be read as a limitation of the concept (choice of quasilocal prescription and slicing/reference), not as a statement about new physics in Schwarzschild.

To interpret the JSON, focus on `local_observables`: `misner_sharp_mass_M_MS` is the baseline constant $M$, while `brown_york_energy_E_BY` varies with $r$ and `E_BY_minus_M_MS` quantifies their disagreement on each sampled sphere. In the provided samples (all exterior, `region: "exterior (r>2M)"`), the sign is positive and the magnitude grows as $r$ approaches $2M$, with the summary `max_abs_E_BY_minus_M_MS_over_valid_samples` capturing the largest such gap; this is the intended “non-uniqueness signal.” Do not over-interpret the absolute values as a physically measured energy profile or as evidence for additional mass; instead, read them as reflecting how $E_{\mathrm{BY}}(r)=r\left(1-\sqrt{1-2M/r}\right)$ assigns different quasilocal bookkeeping than $M_{\mathrm{MS}}(r)=M$ on the same fixed geometry, while the curvature fields (`ricci_scalar`, `kretschmann`) provide context that the spacetime itself remains the standard vacuum Schwarzschild solution.

---

## Toy 066 — Unruh Detector and Observer-Dependent Thermality

This toy illustrates how a simple particle detector responds differently depending on whether it follows an inertial or uniformly accelerated worldline in flat Minkowski spacetime. The physical setup is an Unruh–DeWitt detector with fixed energy gap $\omega$ coupled to a quantum field, and the quantity of interest is the detector’s transition rates between its ground and excited states as a function of proper acceleration $a$. Even though spacetime curvature is identically zero, uniform acceleration defines an effective temperature via $$T_U = a / (2\pi),$$ which sets the scale for the detector’s thermal response.

The toy exists to expose a conceptual pressure point: thermality here is not tied to curvature or gravity, but to the observer’s causal access and motion. It stress-tests the intuition that “temperature requires a heat bath” by showing that the same quantum state of the field yields zero excitation for inertial motion but a Planckian response for accelerated motion. This is not a prediction about real detectors, but a controlled illustration of how observer-dependent coarse-graining leads to detailed balance of the form $\Gamma_{\text{up}}/\Gamma_{\text{down}} = \exp(-2\pi\omega/a)$, highlighting a limitation of interpreting thermodynamic language as purely geometric.

The JSON results should be read by comparing how rates change with acceleration across the sampled points. For $a=0$, the excitation rate is zero while the de-excitation rate is finite, whereas for $a>0$ the excitation rate grows and the ratio of up to down transitions closely matches the expected Boltzmann factor, with deviations only at numerical precision. The absolute magnitudes are less important than the trends and ratios: agreement between the reported detailed balance ratio and $\exp(-\omega/T_U)$ confirms the thermal structure encoded by the equations. What should not be over-interpreted is any notion of real radiation or energy density in spacetime itself—the JSON values operationally encode detector response, not a physical thermal bath filling Minkowski space.

---

## Toy 067 — State-Dependent Stress Tensor on Fixed Schwarzschild Geometry

This toy illustrates how semiclassical stress–energy observables depend on quantum state even when the classical spacetime geometry is completely fixed. The setup is a two-dimensional reduction of Schwarzschild spacetime with metric function $f(r)=1-2M/r$, probed by a massless conformal scalar field. The quantity of interest is the renormalized expectation value of the stress tensor $\langle T_{ab}\rangle$, expressed in null components and combined into simple diagnostics such as the static-frame energy density, which together capture local energy content and flux on this fixed background.

The toy exists to expose a conceptual limitation rather than to make a physical prediction: specifying the geometry alone does not uniquely determine semiclassical observables. Different standard quantum states on the same metric (Boulware, Unruh, and Hartle–Hawking) are encoded through constant offsets $(t_u,t_v)$ in the Polyakov stress tensor, schematically $\langle T_{uu}\rangle=\text{(geometric terms)}+t_u$. The code treats this as a stress test of state-dependence, demonstrating an intrinsic ambiguity in mapping geometry to stress–energy in semiclassical gravity, not a failure of the underlying equations.

The JSON output should be read comparatively across states and radii. For each sampled radius, the per-state values of $\langle T_{uu}\rangle$, $\langle T_{vv}\rangle$, $\langle T_{uv}\rangle$, and the derived $\rho_{\text{static}}=\langle T_{tt}\rangle/f$ show how signs and magnitudes shift with the chosen state, while the ensemble variance fields quantify the spread across the three-state family. The key trend is that this variance grows toward the horizon as $f(r)\to 0$, signaling increased state sensitivity; it should not be over-interpreted as a physical instability or backreaction effect. These numbers simply connect the state-dependent constants to the stress tensor formulas, illustrating how the same equations yield different outcomes under different quantum state choices.

---

## Toy 068 — Semiclassical Backreaction and Thick Failure of Schwarzschild

This toy illustrates a slowly evaporating black hole by treating the Schwarzschild mass as a time-dependent quantity $M(t)$ rather than a fixed parameter. The physical setup is deliberately simple: a spherically symmetric black hole losing mass according to a phenomenological Hawking-like flux, with the horizon radius, temperature, luminosity proxy, and curvature at the horizon recomputed from $M(t)$. The central quantity of interest is the evolving mass itself, governed by the single relation $$\frac{dM}{dt} = -\frac{\alpha}{M^2}$$ where $\alpha$ is a positive constant encoding effective field content and greybody factors.

The toy exists to expose a conceptual limitation rather than to make a physical prediction. Exact Schwarzschild geometry assumes a constant mass, but once even a minimal semiclassical backreaction is introduced, that assumption fails continuously rather than abruptly. The model stresses that there is no sharp geometric signal announcing the breakdown of validity; instead, small changes in flux parameters lead to progressively larger divergences in mass history and remaining lifetime as $M$ decreases. This behavior is framed as a stress test of the quasi-static approximation and of general relativity’s lack of an internal ultraviolet completion, not as evidence for the true endpoint of evaporation.

The JSON output should be read as a time series of snapshots along this controlled drift. As time increases, $M$ decreases smoothly, the horizon radius $r_h = 2M$ shrinks, the Hawking temperature $T_H = 1/(8\pi M)$ rises, and the horizon Kretschmann scalar $K_h = 3/(4M^4)$ grows rapidly, signaling increasing curvature. The sensitivity diagnostics compare a baseline evolution to one with a slightly perturbed $\alpha$, and the growing differences in $\Delta M$ and $\Delta r_h$ illustrate the “thickening” of failure near small mass. Values marked null and the absence of a cutoff time in the summary indicate where the model explicitly refuses interpretation; these numbers should not be over-interpreted as real dynamics, but only as bookkeeping tied back conceptually to the simple mass-loss law and derived formulas.

---

## Toy 069 — Flat-Space Entanglement Area-Law Diagnostic

This toy computes the ground-state entanglement entropy of a local quantum-field-theory analogue in flat spacetime: a 2D square lattice of coupled harmonic oscillators representing a discretized free massive scalar field. The physical setup is a finite $N\times N$ lattice with open boundaries and stiffness matrix $K$ encoding the mass gap and nearest-neighbor couplings, and the quantity of interest is the von Neumann entropy $S$ of a contiguous $L\times L$ subregion after tracing out the rest of the lattice. In the Gaussian ground state the reduced state is fully determined by the restricted covariances, with $S$ obtained from symplectic eigenvalues derived from $C_x=\tfrac{1}{2}K^{-1/2}$ and $C_p=\tfrac{1}{2}K^{+1/2}$.

The toy exists to demonstrate a pressure point in “entropy $\sim$ area” intuition: boundary-dominated entanglement can arise in a flat, horizon-free system purely from locality and tracing across an interface, so an area-law-like trend is not, by itself, evidence of black-hole thermodynamics or spacetime curvature. Conceptually it stress-tests the ambiguity between (i) genuine gravitational interpretations and (ii) generic QFT entanglement structure, by showing that the same kind of scaling can appear with curvature invariants set to zero and no horizons present. The limitation being illustrated is that a relationship of the form $S\propto|\partial A|$ (with $|\partial A|$ the subregion boundary length) is a structural feature of local ground states rather than a physical prediction about gravity.

To interpret the JSON, focus on how `local_observables.entanglement_entropy_S` changes with `region_size_L` and how stable `entropy_per_boundary_length_S_over_4L` remains as `boundary_length_perimeter_4L=4L` grows. In the provided run ($N=8$, $m=0.5$), $S$ increases with $L$ while $S/(4L)$ decreases only mildly from about $0.0395$ at $L=1$ to about $0.0259$ at $L=4$, whereas the summary ranges show $S/L^2` varying much more strongly (`S_over_area_max` to `S_over_area_min`), which is the qualitative signature the toy is designed to expose. What should not be over-interpreted is the exact numerical value of the prefactors, small-$N$ deviations, or any claim of gravitational entropy: the fields `curvature_invariants` and `causal_structure` explicitly indicate flat background and no horizon, and the reported entropies are meant to be read as outcomes of the Gaussian-covariance procedure that conceptually implements $S=\sum_i[(\nu_i+\tfrac{1}{2})\ln(\nu_i+\tfrac{1}{2})-(\nu_i-\tfrac{1}{2})\ln(\nu_i-\tfrac{1}{2})]$ rather than as thermodynamic observables.

---

## Toy 070 — Patchwork reconstruction failure: global state not determined by local reduced data

This toy builds a simple “flat” quantum field model as an $N\times N$ lattice of coupled harmonic oscillators (a discretized massive free scalar), then treats its ground state as a Gaussian state specified by position and momentum covariances. The global setup is encoded by $H=\tfrac12(p^T p+x^T K x)$ with $K=m^2 I+\mathrm{Laplacian}_{2D}$, and the quantity of interest is whether the full-lattice covariances can be recovered from only local reduced covariances on overlapping $L\times L$ patches. In the code’s convention $\hbar=1$, the true ground-state covariances are $C_x=\tfrac12 K^{-1/2}$ and $C_p=\tfrac12 K^{+1/2}$, which serve as the “ground truth” global data against which any reconstruction is compared.

The toy exists to illustrate a limitation of “reduction by patches”: even when the background is operationally trivial (curvature invariants are set to zero and there are no horizons), local reduced data plus an apparently reasonable stitching rule need not fix the global state in a principled way. It demonstrates a specific failure mode by adopting a deterministic but naive reconstruction rule—entrywise averaging of all patch submatrix values that cover each global matrix element—and then showing that this can produce ambiguity (many global states can share the same local reductions) and inconsistency (the stitched object can drift away from a globally valid covariance). The stress test is sharpened by a physicality proxy: for a Gaussian state without $x$–$p$ cross-covariances, a mode-wise uncertainty check expects eigenvalues of $C_x C_p$ to satisfy $\min\!\operatorname{eig}(C_x C_p)\ge 1/4$, and the reconstruction can approach or violate this bound even though the underlying true state is well-behaved.

To interpret the JSON, focus on how reconstruction quality and “physicality margin” change with patch size $L$: `relative_frobenius_error_Cx` and `relative_frobenius_error_Cp` quantify how far the stitched covariances are from the true $C_x$ and $C_p$, while `min_eigenvalue_CxCp_reconstructed` and `heisenberg_modewise_ok_reconstructed` summarize whether the stitched pair behaves like a plausible Gaussian covariance under the proxy $\min\!\operatorname{eig}(C_x C_p)\ge 1/4$. In the provided run ($N=8$, $m=0.5$), errors drop rapidly as $L$ grows and become essentially zero at $L=8$ (the single patch is the whole lattice), but intermediate sizes can still fail the uncertainty proxy (e.g., $L=2,3,4,6$ have $\min\!\operatorname{eig}(C_x C_p)<1/4$), which is the signature the toy is designed to expose. What should not be over-interpreted is any “physical prediction” from the reconstructed numbers: the point is not that the lattice ground state is unphysical, but that the mapping from local reduced covariances to a global object depends on extra assumptions beyond the local data, so the JSON values should be read as diagnostics of the stitching rule relative to the anchors $C_x=\tfrac12 K^{-1/2}$, $C_p=\tfrac12 K^{+1/2}$ and the conceptual check on $C_x C_p$, not as statements about nature.

---

