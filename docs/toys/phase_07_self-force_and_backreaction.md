# Phase 7 — Self-Force & Backreaction

**Toys 034–040**

---

## Toy 034 — Radiation Reaction as a Self-Force Proxy in an EMRI Inspiral

This toy illustrates how a small body of mass $\mu$ orbiting a much larger black hole of mass $M$ gradually inspirals when gravitational-wave emission is taken into account. The physical setup is a quasi-circular orbit in a fixed Schwarzschild background, where the quantity of interest is the slow, secular change of the orbital radius over time rather than the instantaneous geodesic motion. At each radius the orbit is treated as a circular geodesic with specific energy $Ẽ(r)=(1-2M/r)/\sqrt{1-3M/r}$, while energy loss to gravitational waves drives the inspiral through an effective radial velocity.

The toy exists to expose a limitation of the common statement “free fall equals geodesic motion” by stressing that this is only exact in the strict test-mass limit. Once $\mu$ is finite, even if $\mu\ll M$, the body interacts with its own gravitational radiation and its worldline deviates from a geodesic sequence. Rather than predicting a physically complete inspiral, the model deliberately combines exact geodesic energetics with a leading-order quadrupole flux to form a controlled proxy, using the balance law $dr/dt=-P/(dE/dr)$, to demonstrate how radiation reaction acts as an effective self-force and where simple approximations begin to strain.

The JSON output should be read as a time-ordered record of this adiabatic evolution. The key trends are that $r$ decreases monotonically, $dr/dt$ is negative and grows in magnitude, and both the orbital frequency and gravitational-wave power increase as the orbit shrinks, consistent with the scaling $P\propto r^{-5}$. Magnitudes and smoothness matter more than exact values: the results show a secular inspiral and accumulated orbital phase, not a precise waveform or strong-field prediction. One should not over-interpret the late-time behavior near small radii, where neglected relativistic flux corrections and conservative self-force effects would become important; instead, the listed energies, frequencies, and fluxes should be connected back conceptually to the energy-balance equation as indicators of how self-interaction alters motion away from ideal geodesics.

---

## Toy 035 — When Linearized Gravity Stops Being Self-Consistent

This toy computes the leading-order (linearized) tidal field of a plane, plus-polarized gravitational wave traveling in the $+z$ direction on an otherwise flat spacetime, using the TT-gauge perturbation $h_{xx}=A\cos\phi$ and $h_{yy}=-A\cos\phi$ with phase $\phi=\omega(t-z)$ (where $A$ is the dimensionless strain amplitude and $\omega$ is the angular frequency). The quantity of interest is the wave’s “electric” tidal tensor components $E_{ij}=R_{0i0j}$ at $O(h)$, summarized by a scalar norm built from $R_{0x0x}$ and $R_{0y0y}$; in this model the representative component is $$R_{0x0x}=\frac{A\omega^2}{2}\cos\phi.$$ Conceptually, the toy answers: given a chosen amplitude and sampling points $(t,z)$, what linearized tidal magnitude would you report, and what is a simple indicator of whether that report is even internally consistent?

The toy exists to expose a specific failure mode: linearized GR keeps only $O(h)$ terms and discards $O(h^2)$, but GR’s dynamics and stress-energy bookkeeping are nonlinear, so large perturbations are not just “bigger waves” but qualitatively beyond the approximation. Instead of claiming a physical prediction for strong waves, it uses an order-of-magnitude consistency criterion—treating the ratio of neglected to kept terms as roughly the size of the perturbation itself—captured by the proxy $$\text{nonlinearity\_ratio\_proxy}\approx |h|=|A\cos\phi|.$$ The point is a stress test: the linear tidal formula can return perfectly finite numbers even when the approximation it came from is no longer controlled, so the toy makes that breakdown explicit rather than implicit.

To interpret the JSON, read each `sample_points` entry as a location $(t,z)$ with its computed phase `phase_phi`, the local perturbations `h_xx`, `h_yy`, and the linearized tidal components `R0x0x_linear`, `R0y0y_linear` whose magnitudes oscillate with $\cos\phi$ and scale linearly with $A$ (and as $\omega^2$ through $R_{0x0x}=(A\omega^2/2)\cos\phi$). The scalar `tidal_norm_linear` tracks the overall tidal strength (it grows with $|A\cos\phi|$), while `nonlinearity_ratio_proxy_|h|` is the key “validity dial”: values $\ll 1$ indicate a controlled linear regime, values near $10^{-1}$ are borderline, and values $\gtrsim 0.1$ (flagged as `nonlinear_expected`) mean you should not over-interpret the linear tidal numbers as quantitatively reliable. The `kretschmann_proxy` is only a comparability diagnostic built from the tidal magnitude, so you should not read it as a true curvature invariant for an exact plane wave; instead, treat it as a monotone proxy that mirrors the same scaling logic as the tidal components and the $|h|$ validity proxy.

---

## Toy 036 — Energy Conditions vs Constant-w Perfect Fluids

This toy evaluates how the standard energy conditions behave for simple cosmological matter models in a flat Friedmann–Lemaître–Robertson–Walker spacetime. The physical setup is a homogeneous and isotropic universe filled with a single perfect fluid obeying a constant equation of state $p=w\rho$, where $\rho$ is the energy density and $p$ the pressure. Using the $k=0$ Friedmann relation $\rho = 3H^2/(8\pi)$, the toy computes the expansion rate, curvature invariants, and the combinations of $\rho$ and $p$ that enter the null, weak, strong, and dominant energy conditions at sampled times.

The toy exists to expose a conceptual limitation rather than to make a physical prediction: many foundational results in general relativity assume energy conditions that are not enforced by the theory itself. By varying $w$, the toy stresses how commonplace effective matter models systematically violate these assumptions, for example when acceleration requires $\rho+3p<0$ or when phantom-like fluids give $\rho+p<0$. This demonstrates that the failure of an energy condition reflects properties of the chosen stress–energy model, not an inconsistency of the spacetime geometry or the field equations.

The JSON output should be read as a diagnostic map over $(t,w)$ samples, indicating which energy conditions are satisfied and how often violations occur. Signs and magnitudes of $\rho+p$, $\rho+3p$, and $|p|$ relative to $\rho$ determine the boolean flags, while the summary fractions quantify how generic each violation is across the sample. Curvature scalars and expansion rates provide context but should not be over-interpreted as evidence for singular behavior or realism; they simply reflect the same inputs through relations like $R=6(\dot H+2H^2)$ and are included to connect the stress–energy diagnostics back to the underlying FLRW dynamics.

---

## Toy 037 — Curvature Scale vs Cutoff Length in Schwarzschild

This toy samples the Schwarzschild vacuum spacetime for a chosen mass $M$ (in geometric units with $G=c=1$) and reports how “large” the curvature is at different radii $r$ by using the Kretschmann invariant $K=48 M^2/r^6$. From $K$ it builds a curvature length scale $L_K=K^{-1/4}$ (where $L_K$ is the characteristic length associated with the local curvature), and then compares that length to a user-supplied minimum length cutoff $\ell_{\min}$. The quantity of interest is the dimensionless UV/classicality parameter $\chi(r)=\ell_{\min}/L_K=\ell_{\min} K^{1/4}$, which is intended to indicate whether the sampled region looks “smooth” relative to the imposed cutoff.

The toy exists to expose a limitation of classical GR as a modeling language: the equations happily admit regions where curvature grows without any intrinsic UV regulator, so nothing inside the classical framework warns you that a smooth-spacetime description may stop being operationally meaningful. By introducing an explicit $\ell_{\min}$ (standing in for a Planck length once $\hbar$ is fixed, an EFT cutoff, or an instrumental resolution), the toy turns that conceptual ambiguity into a concrete stress test: it flags when the curvature-implied length $L_K$ becomes comparable to the cutoff. In this framing, the marker $\chi \sim 1$ is not a physical prediction about “what really happens,” but a diagnostic boundary indicating that the classical continuum description is being pushed to (or beyond) the scale it is assumed to describe.

To interpret the JSON, focus on how $\chi_{\mathrm{uv}}$ changes with $r$ and where it approaches or exceeds unity: for fixed $M$ it should grow rapidly as $r$ decreases (because $K \propto r^{-6}$ so $\chi \propto r^{-3/2}$), and the summary field `closest_to_chi_eq_1` highlights the sampled radius where $\chi$ is nearest 1 (here near $r=0.02$ with $\chi \approx 0.93$, meaning $L_K$ is close to $\ell_{\min}$). The per-point `curvature_length_L_K` and `kretschmann` fields let you see the same story in length and invariant form: decreasing $L_K$ and increasing $K$ should track the rise of $\chi$ through $\chi=\ell_{\min} K^{1/4}$. What should not be over-interpreted are the categorical `regime` labels or the interior/horizon/exterior `region` tags as statements about actual quantum gravity, singularity resolution, or dynamics—this output only illustrates when the chosen cutoff $\ell_{\min}$ becomes comparable to the curvature length scale implied by the classical invariant.

---

## Toy 038 — Semiclassical Backreaction as Effective Mass Loss

This toy illustrates a simplified model of black hole evaporation by replacing a stationary Schwarzschild spacetime with a time-dependent mass parameter $M(t)$. Instead of deriving radiation from quantum fields, the model prescribes an effective mass-loss law that mimics Hawking evaporation, $dM/dt = -\alpha/M^2$, where $\alpha$ is a fixed positive constant. From this evolving mass, the toy computes horizon radius, a Hawking-temperature proxy, luminosity, and curvature at the horizon, all treated as functions of time rather than predictions of a full quantum-gravitational theory.

The toy exists to expose a conceptual limitation of classical general relativity when confronted with semiclassical effects. Classical GR admits stationary black hole solutions with no intrinsic mass loss, while Hawking radiation requires a quantum stress-energy tensor that violates classical energy conditions near the horizon. By inserting the mass-loss law by hand, the toy stress-tests how far one can push a classical geometric description once quantum backreaction is assumed rather than derived, emphasizing that the evolution is an imposed proxy rather than a self-consistent solution of Einstein’s equations with a known source.

The JSON output should be read as a time series of diagnostic proxies tied to the assumed evolution $M(t) = (M_0^3 - 3\alpha t)^{1/3}$ up to the evaporation time. The important trends are monotonic: decreasing mass, shrinking horizon radius, increasing temperature proxy $T_H \sim 1/M$, and rapidly growing horizon curvature $K_h \sim 1/M^4$, alongside an increasing semiclassicality proxy $\epsilon_{\mathrm{sc}} \sim 1/M^2$. These quantities illustrate how the model is driven toward a regime where semiclassical and then quantum-gravity effects would dominate, but none of the magnitudes or late-time behavior should be over-interpreted as physical predictions beyond the breakdown of the proxy itself.

---

## Toy 039 — Cosmological Backreaction from Averaging

This toy constructs a minimal “universe” made of two disjoint comoving regions, each of which expands exactly like a flat FLRW model with a power-law scale factor, but with different expansion exponents. The quantity of interest is the domain-averaged expansion, defined by first averaging volumes and only then defining an effective scale factor and Hubble rate. Concretely, if the regional scale factors are $a_i(t)=(t/t_0)^{n_i}$, the averaged volume is $V_D=f_0 a_1^3+(1-f_0)a_2^3$ and the averaged scale factor is $a_D=V_D^{1/3}$, from which an effective expansion rate is inferred.

The purpose of the toy is not to predict cosmological dynamics, but to expose a conceptual weakness in “averaging the universe” in general relativity. Even when each region is perfectly homogeneous and isotropic on its own, averaging and time evolution do not commute, so the averaged dynamics need not satisfy the same equations as either region. This failure is encoded in a backreaction term $Q_D=(2/3)(\langle\theta^2\rangle-\langle\theta\rangle^2)$, where $\theta_i=3H_i$, which arises purely from variance in expansion rates and exists even in the absence of shear or curvature differences. The toy therefore serves as a stress test of the assumption that fitting a single FLRW model to averaged densities captures the true large-scale behavior.

The JSON output should be read as a time series of averaged diagnostics rather than as a single spacetime solution. The key indicator is the backreaction term `Q_backreaction`, which is nonzero whenever the two regions expand at different rates and whose magnitude tracks the variance between $H_1$ and $H_2$ as the volume fractions evolve. Trends in `H_domain`, `a_domain`, and the decreasing dust densities illustrate how the averaged expansion drifts away from what one would infer from density alone, while the curvature scalars are only volume-weighted proxies and should not be interpreted as invariants of a real metric. Small mismatches between the two sides of the effective acceleration equation highlight numerical and conceptual limitations of the averaging procedure and should be taken as demonstrations of noncommutativity, not as physical inconsistencies or evidence for actual cosmic acceleration.

---

## Toy 040 — Constraint propagation and numerical breakdown

This toy evolves a 1D, periodic wave field $h(t,x)$ with a companion field $p(t,x)$ that plays the role of a time-derivative, using the evolution system $h_t=p$ and $p_t=h_{xx}$. Alongside the evolution, it tracks a “constraint monitor” $C(t,x)$ that should vanish in the continuum but becomes nonzero when the discrete representation treats equivalent operations inconsistently; here, $C$ is defined as the difference between two numerical estimates of the same spatial derivative of $p$. The quantity of interest is not $h$ itself, but how the norm of $C$ behaves over time as a proxy for constraint health in a constrained evolution setting.

The toy exists to illustrate a common failure mode in numerical-relativity-style systems: even when the continuum equations imply constraints are preserved, discretization and formulation choices can manufacture constraint violations that persist, oscillate, or transiently amplify. In this setup the continuum target is effectively $C=0$ (because the two derivative notions coincide only in the continuum limit), so any nonzero $C$ is a numerical artifact rather than a physical signal. The damped mode stress-tests a generic mitigation idea by modifying the evolution as $p_t=h_{xx}-\kappa\,\partial_x C$, which is not “GR-accurate” but exposes how adding feedback from constraint gradients can change the propagation or decay of violations.

To interpret the JSON, focus on the time series in `sample_points[*].local_observables` for `constraint_L2` and `constraint_Linf`, which are the domain-integrated and peak magnitudes of the monitored violation, respectively. What matters are relative trends (growth, decay, sustained oscillation) and scale separation (e.g., whether $||C||_\infty$ spikes while $||C||_2$ stays modest, indicating localized numerical artifacts) rather than any absolute “physical” size; these norms quantify the magnitude of the discrete mismatch embodied by $C$, not curvature or energy. Do not over-interpret late-time decreases as convergence or correctness without comparing resolutions and modes: the same system can show large early excursions and later relaxation purely from dispersive or aliasing behavior, and the “constraint” being measured is tied conceptually to the continuum anchor $C=0$ while the damping term connects back by trying to drive $\partial_x C$ toward zero through the reported $\kappa$ setting.

---

