# Phase 12 — Holography & Emergence

**Toys 081–085**

---

## Toy 081 — Positive Mass Theorem Boundary: When the Hypotheses Switch Off

This toy compares two simple spatial geometries used in general relativity to highlight when “global energy” statements are meaningful: a time-symmetric $t=\mathrm{const}$ slice of Schwarzschild and a flat 3-torus. On the Schwarzschild slice it samples the lapse-like factor $f(r)=1-2M/r$ (with $M$ the mass parameter) and curvature strength via the Kretschmann invariant $K=48M^2/r^6$, and it also computes a proxy for “how far” the inner end of the slice is by integrating the radial proper distance $D=\int dr/\sqrt{f(r)}$ over regions where $f(r)>0$. The quantity of interest is not a local force or orbit prediction, but whether the setup even provides the global conditions under which an ADM mass $M_{\mathrm{ADM}}$ can be interpreted and compared to positivity theorems.

The purpose is to stress-test the boundary of applicability of the Positive Mass Theorem (PMT) rather than to predict physics: PMT is a global statement that requires hypotheses like asymptotic flatness and geodesic completeness (along with nonnegative scalar curvature on the initial slice). The toy demonstrates two “switch-off” mechanisms: (i) for Schwarzschild with $M<0$, the geometry has a curvature singularity at $r=0$ reached at finite proper distance on the standard slice, so completeness fails and “negative mass” is not a theorem violation but a hypothesis failure; (ii) for a compact flat 3-torus, there is no asymptotic region, so ADM mass is operationally undefined rather than negative. In other words, the toy exposes that “$M_{\mathrm{ADM}}\ge 0$” is a conditional stress test of assumptions, not a universal output of the equations.

To read the JSON, treat the Schwarzschild and torus blocks as two different diagnostics for theorem scope: in `schwarzschild_summary`, `adm_mass_M_ADM` is the parameter-based ADM mass (here $1.0$) and `pmt_applicability_proxy` indicates whether the toy’s completeness/asymptotic-flatness checklist is “on” for the chosen case. The key trend lives in `completeness_boundary_measurement.distance_samples`: for $M>0$ the inner boundary is $r\to 2M^+$ and the reported `proper_distance_r0_to_r_inner` should grow as `epsilon` shrinks, consistent with the integral $D=\int dr/\sqrt{f(r)}$ becoming large near $f(r)\to 0$; the fact that the distances increase from about $10.55$ to $11.79$ as $\epsilon$ goes from $10^{-1}$ to $10^{-4}$ is the sign you should watch, not the exact last decimal. In `sample_points`, the rise of `kretschmann` as $r$ decreases (via $K=48M^2/r^6$) is a local indicator of stronger curvature, but you should not over-interpret it as an “energy density” or as validating PMT by itself; likewise, `torus_summary.adm_mass_M_ADM: null` should be read as “not defined” (no asymptotic region), not as “zero” or “positive.”

---

## Toy 082 — Swampland Distance Conjecture EFT Boundary

This toy illustrates how an effective field theory based on general relativity coupled to a scalar modulus can lose predictivity as the field moves far in moduli space. The setup considers a canonically normalized scalar field $\phi$ whose displacement $\Delta\phi$ controls the mass of a tower of additional states, taken to decrease exponentially as $m(\phi)=m_0\exp(-\alpha\Delta\phi)$, where $m_0$ is an initial mass scale and $\alpha$ sets the rate of descent. The quantity of interest is not a spacetime curvature itself, but the effective ultraviolet cutoff $\Lambda_{\mathrm{eff}}$ of the theory, which is compared to a fixed curvature or energy scale $\kappa$ to judge whether the low-energy description remains meaningful.

The toy exists to expose a conceptual boundary implied by the Swampland Distance Conjecture rather than to make a physical prediction. As the tower of states becomes light, the number of species below a given scale grows and the species bound lowers the cutoff, schematically $\Lambda_{\mathrm{eff}}\sim M_{\mathrm{pl}}/\sqrt{N}$ with $N\sim(M_{\mathrm{pl}}/m)^p$, where $M_{\mathrm{pl}}$ is the Planck mass and $p$ is a proxy exponent. The stress test demonstrated here is that even in the absence of large geometric curvature, the effective field theory can fail because its own cutoff collapses below the scales it is meant to describe, illustrating a limitation of GR-as-EFT rather than a violation of equations of motion.

The JSON output should be read as a scan over field values $\phi$, with each sample point reporting the tower mass, the resulting species cutoff, and a boolean proxy for EFT validity based on the simple criterion $\Lambda_{\mathrm{eff}}>\kappa$. Trends matter more than individual numbers: as $\Delta\phi$ increases, $m$ decreases smoothly and $\Lambda_{\mathrm{eff}}$ tracks this descent, potentially approaching the comparison scale. A transition where the validity flag would flip from true to false would mark the illustrative breakdown point, but its precise location should not be over-interpreted as a sharp physical threshold or evidence for the conjecture itself. Instead, the JSON values conceptually encode how the exponential mass relation feeds into the species-bound cutoff and how that cutoff compares to $\kappa$, highlighting the regime where the effective description becomes operationally undefined.

---

## Toy 083 — SYK/AdS Geometry Emergence as an Operational Threshold

This toy evaluates whether a strongly interacting quantum many-body state supports a *stable geometric interpretation* by combining two diagnostics: the shape of a thermal two-point function and the amount of ground-state entanglement. The setup is a deterministic, SYK-like model of $n$ complex fermion modes with a quartic interaction, analyzed by exact diagonalization. The central observable is the Euclidean thermal correlator $G(\tau)=(1/n)\sum_i\langle c_i(\tau)c_i^\dagger(0)\rangle_\beta$, which is compared against a fixed conformal kernel shape expected in low-energy SYK lore, together with the entanglement entropy of a half-system bipartition.

The toy exists to expose a limitation in common “emergent geometry” narratives: neither strong interactions nor high entanglement alone define a unique or even stable notion of geometry. Instead of predicting spacetime, the toy stress-tests an operational proxy that only declares emergence when two independent signals stabilize simultaneously. Concretely, it checks whether the correlator is well approximated by a conformal form $K(\tau)=((\pi/\beta)/\sin(\pi\tau/\beta))^{2\Delta}$ with small relative error, and whether the ground state approaches maximal half-system entanglement. The absence or delay of this coincidence illustrates a thick crossover and an operationally undefined regime rather than a sharp phase boundary.

The JSON output should be read comparatively across $(n,J)$ values rather than as absolute statements. The key quantities are the relative RMSE of the conformal fit, the fitted amplitude $b$, and the entanglement fraction $S_A/((n/2)\ln 2)$. In the provided results, increasing coupling strength generally improves the conformal fit (lower fit error) but does not raise the entanglement fraction above the chosen tolerance, so the combined geometry-emergent proxy remains false in all cases. This should not be over-interpreted as the absence of geometry; it only demonstrates that, under this proxy and these finite-size conditions, correlator conformity and near-maximal entanglement do not coincide. The JSON values therefore illustrate how the proxy operationalizes the equations above, not a claim about physical spacetime.

---

## Toy 084 — Final-state obfuscation in a unitary evaporation toy

This toy simulates an abstract “black hole + radiation” system built from qubits, with a one-qubit reference $R_f$ initially maximally entangled with a single black-hole qubit, and the rest initialized to $|0\rangle$. At each step a deterministic scrambling unitary acts on the black-hole register, then one qubit is emitted into the radiation register via a swap, so the model is globally unitary while the radiation subsystem can look thermalized. The main quantity of interest is how mixed the emitted radiation appears as evaporation proceeds, summarized by the von Neumann entropy $S(\rho)=-\mathrm{Tr}(\rho\log\rho)$ for the reduced radiation state $\rho_{\mathrm{Rad}}$.

The toy exists to stress-test a conceptual loophole: “unitarity preserved” does not automatically imply operational predictability for observers who do not control boundary conditions. It illustrates how a final-state (postselected) boundary condition imposed on the remaining black-hole degrees of freedom at the end of evolution can force the overall outcome to be compatible with a pure global story, yet make concrete predictions depend sharply on which final boundary was chosen and on whether postselection succeeds. The stress test is encoded by comparing radiation states using the trace distance $D(\rho,\sigma)=\tfrac{1}{2}\|\rho-\sigma\|_1$, which measures how distinguishable two predicted radiation density matrices are under different final-state assumptions.

To interpret the JSON, read the per-step “page_curve_proxy” values as diagnostics of subsystem mixing: increasing $S_{\mathrm{rad}}$ indicates radiation becoming more entangled with what remains, while the “information_return_proxy” mutual information tracks how strongly the radiation correlates with the reference $R_f$ via $I(\mathrm{Rad}:R_f)=S(\mathrm{Rad})+S(R_f)-S(\mathrm{Rad}R_f)$. The “final_state_obfuscation” block then quantifies end-of-evaporation sensitivity: small postselection success probabilities mean the enforced final boundary is operationally rare, and large trace distances—especially between the two postselected radiation states—mean different final-state choices yield sharply different radiation predictions even though the underlying evolution is unitary. What should not be over-interpreted is any literal claim about real black holes or spacetime dynamics: magnitudes here are qubit-scale information-theory markers, and the key point is the relationship between entropy/mutual-information diagnostics and the distinguishability measures, not a physical forecast.

---

## Toy 085 — Cross-Phase Comparator for Failure Density and Mode Coverage

This toy does not simulate a spacetime directly; instead, it ingests the JSON outputs of many other toys and treats them as data. The physical setup is therefore meta-level: a collection of toy models, each with its own assumptions and regimes, is viewed as an ensemble to be summarized and compared. The central quantity of interest is a proxy for “operational undefinedness,” defined as the fraction of leaf values in each JSON payload that are explicitly null, which can be written schematically as $$f_{\mathrm{null}} = N_{\mathrm{null}} / N_{\mathrm{leaf}}$$ where $N_{\mathrm{leaf}}$ is the total number of scalar entries and $N_{\mathrm{null}}$ counts those that have gone undefined.

The toy exists to illustrate a limitation of large toy suites: without instrumentation, claims about coverage and robustness remain qualitative. Here the conceptual issue is not a breakdown of general relativity, but a breakdown of the laboratory itself as it scales—missing schema elements, silent regime exits, and inconsistent labeling of failure modes become invisible without a comparator. Toy 085 frames this as a stress test of the lab’s bookkeeping, using crude keyword scans and null counts to expose ambiguity and incompleteness, rather than as any statement about physical pathologies in the underlying models.

The JSON results should be read as diagnostic summaries. Per-toy entries report schema compliance, the null fraction (its magnitude and variation across toy number or phase are what matter), and binary indicators of whether certain failure-mode keywords ever appear. Aggregate fields such as phase diagrams and means or maxima of $f_{\mathrm{null}}$ indicate where undefinedness or particular modes cluster, but they should not be over-interpreted as quantitative measures of physical severity or likelihood. Instead, these values conceptually map back to the definition of $f_{\mathrm{null}}$ and to the coarse keyword presence counts, serving only to highlight trends and gaps in how the ensemble of toy equations and outputs has been specified and annotated.

---

