# Phase 11 — Quantum Observers & Emergent Causality

**Toys 071–080**

---

## Toy 071 — de Sitter horizon thermality as vacuum- and observer-dependent “particle content”

This toy samples a stationary Unruh–DeWitt–type detector in the static patch of 4D de Sitter spacetime with metric $ds^2=-(1-H^2 r^2)\,dt^2+(1-H^2 r^2)^{-1}dr^2+r^2 d\Omega^2$, where $H=\sqrt{\Lambda/3}$ sets the horizon radius $r_H=1/H$ and the global Gibbons–Hawking scale $T_{dS}=H/(2\pi)$. The quantity of interest is the detector’s operational “thermality,” encoded as a detailed-balance ratio between absorption and emission for a level spacing $\omega>0$. In the Bunch–Davies state the toy models this as thermal with a locally redshifted temperature $T_{\mathrm{loc}}(r)=T_{dS}/\chi(r)$, where $\chi(r)=\sqrt{1-H^2 r^2}$ is the static redshift factor; in the static-patch vacuum the toy idealizes the same stationary detector as seeing no bath.

The toy exists to expose a limitation in treating horizon thermality as an intrinsic, observer-independent “prediction of curvature.” De Sitter has constant scalar invariants (e.g. the Ricci scalar and Kretschmann scalar are the same at all $r$), yet the inferred particle content can change with both the observer’s worldline (through $\chi(r)$) and the vacuum choice (Bunch–Davies versus static-patch time). The stress test is that the same geometry and the same detector model can yield a thermal detailed-balance ratio in one state and an effectively zero absorption signature in another, demonstrating that “particles” here are contextual rather than a uniquely defined property of the background. The operational principle the toy leans on is $R_{\mathrm{abs}}/R_{\mathrm{emit}}=\exp(-\omega/T_{\mathrm{loc}})$ for a stationary detector in a thermal state, contrasted with the idealized suppression $R_{\mathrm{abs}}/R_{\mathrm{emit}}\approx 0$ in the static-patch vacuum.

To interpret the JSON, focus on how the Bunch–Davies entries change with radius and gap: `chi_redshift_factor` decreases as $r$ approaches the horizon, so `temperature_local` increases, and correspondingly the Bunch–Davies `detector_detailed_balance_ratio_Rabs_over_Remit` becomes less extremely small at fixed $\omega$ (because $\exp(-\omega/T_{\mathrm{loc}})$ grows as $T_{\mathrm{loc}}$ rises). For fixed $r$, increasing `detector_gap_omegas` drives that ratio down rapidly, reflecting the same exponential dependence on $\omega/T_{\mathrm{loc}}$ rather than any dynamical instability. The `static_patch_vacuum` ratios are exported as exactly `0.0` for $\omega>0$ by construction, so their role is a qualitative comparator, not a numerically meaningful limit or a statement about detector microphysics near the horizon. Do not over-interpret absolute magnitudes as physical rates or as evidence of real radiation; instead, read the trends as illustrating how the JSON’s temperature fields conceptually feed the exponential detailed-balance relation and how changing vacuum choice can flip the operational “particle content” even when curvature invariants remain constant.

---

## Toy 072 — Vacuum Energy Catastrophe (Zero-Point Cutoff vs Observed Λ)

This toy compares two ways of assigning an energy density to “empty space.” On the QFT side it uses a flat-space, massless-field proxy for zero-point energy with a hard UV cutoff $k_{\max}$, giving a vacuum energy density $\rho_{\mathrm{vac}}=(\hbar c/(16\pi^2))\,k_{\max}^4$, where $\hbar$ is Planck’s constant, $c$ is the speed of light, and $k_{\max}$ is the maximum wave number included. On the GR side it treats the observed cosmological constant $\Lambda$ as an effective energy density $\rho_\Lambda$ that would source curvature if it gravitated normally, so the “quantity of interest” is not curvature itself but the scale mismatch between $\rho_{\mathrm{vac}}$ and $\rho_\Lambda$.

The toy exists to stress-test the assumption that naive vacuum-energy bookkeeping can be coupled to gravity without extra structure. The conceptual issue it exposes is that a cutoff-based estimate of zero-point energy grows as $k_{\max}^4$, while the observed $\Lambda$ corresponds to an energy density $\rho_\Lambda=(c^4\Lambda)/(8\pi G)$ (with $G$ Newton’s constant) that is tiny by comparison; for Planckian choices of $k_{\max}$ this creates an extreme incompatibility. This is not a physical prediction of what “the vacuum really is,” but a diagnostic that “QFT vacuum energy gravitates straightforwardly” plus “a naive UV cutoff is meaningful” is an internally fragile combination, illustrating a limitation/failure mode rather than forecasting cosmology.

In the JSON, each `sample_points[i]` corresponds to a chosen cutoff scale `cutoff_scale_alpha = α`, where the toy sets $k_{\max}=\alpha/\ell_p$ and $\ell_p=\sqrt{\hbar G/c^3}$ is reported as `planck_length_m`. The key readouts are `rho_vac_QFT_J_per_m3`, `rho_Lambda_obs_J_per_m3`, and their ratio `ratio_rho_vac_over_rho_Lambda`: the only trend that matters is the quartic scaling with α (increasing α by 10 should raise the ratio by $\sim 10^4$), and the only magnitude that matters is that the ratio is astronomically large (here spanning $\sim 10^{116}$ to $\sim 10^{124}$ over α = 0.1, 1, 10). You should not over-interpret the exact prefactor, the choice of field content, or the cutoff model as “measured physics”; instead, interpret the reported values as a bookkeeping-to-coupling consistency check linking the two anchor equations by showing how the $k_{\max}^4$ growth in $\rho_{\mathrm{vac}}$ translates directly into the huge ratios relative to $\rho_\Lambda$.

---

## Toy 073 — Big Bang singularity in flat FLRW: invariant blow-up and ill-defined initial data

This toy evaluates curvature invariants in a spatially flat ($k=0$) Friedmann–Lemaître–Robertson–Walker (FLRW) spacetime with metric $ds^2=-dt^2+a(t)^2(dx^2+dy^2+dz^2)$, using two standard early-time power laws normalized by $a(1)=1$: radiation domination $a(t)=t^{1/2}$ and matter domination $a(t)=t^{2/3}$. The quantity of interest is not the scale factor itself but a coordinate-invariant diagnostic of geometric breakdown: the Ricci scalar $R$ and the Kretschmann scalar $K$, computed from $H=\dot a/a$ via $R=6(\ddot a/a+H^2)$ and $K=12((\ddot a/a)^2+(H^2)^2)$ for $k=0$. By sampling progressively smaller positive times $t$, the toy illustrates how these invariants behave as the model approaches the putative initial boundary at $t=0$.

The toy exists to stress-test classical predictability in cosmological symmetry reductions: even when the metric looks benign in comoving coordinates, the geometry can terminate at finite proper time in a way that cannot be removed by a coordinate change. The conceptual issue it exposes is that classical “initial conditions at $t=0$” are not well-defined inside this framework because invariant measures of curvature become unbounded as $t\to0^+$, so extending the solution through $t=0$ would require new non-GR input rather than better numerics. In this sense, the blow-up trend $K\propto t^{-4}$ (and typically $R\propto t^{-2}$ when nonzero) is a limitation/edge-case signal for the classical model, not a physical prediction about an actual beginning.

To interpret the JSON, treat each entry in `sample_points` as a snapshot at a given `coordinates.t` and `coordinates.model`, with `curvature_invariants.ricci_scalar` and `curvature_invariants.kretschmann` providing the invariant “stress indicators” and `local_observables` giving the corresponding $a(t)$, $H(t)$, and $\ddot a/a$ that feed those invariants. The key trend is the rapid growth of `kretschmann` as `t` decreases (roughly multiplying by $16$ when $t$ halves, consistent with $K\sim t^{-4}$), and the fact that this happens for both radiation and matter models, while `ricci_scalar` differs (it is approximately zero for the radiation power law here, but grows like $t^{-2}$ for the matter power law). The `energy_density_proxy` fields (`rho_r0/a^4` for radiation and `rho_m0/a^3` for matter with `rho_*0=1`) should be read only as scaling proxies that track how the chosen $a(t)$ amplifies densities; their absolute values and thresholds (including the summary flag `singularity_confirmed_by_invariant_blowup`) should not be over-interpreted as a physical criterion. Conceptually, the JSON’s divergent magnitudes are meaningful only insofar as they reflect the same underlying structure in the equations—$H\sim 1/t$ and $\ddot a/a\sim 1/t^2$ feeding $R$ and $K$—showing that the approach to $t=0$ is an invariantly ill-behaved boundary for the classical toy model.

---

## Toy 074 — Quantum observer superposition and operationally undefined time dilation

This toy illustrates a simple but sharp operational question in curved spacetime: what geometric quantity is “measured” if the observer itself is in a quantum superposition of locations. The physical setup is a fixed Schwarzschild spacetime of mass $M$, with two possible static observer radii $r_A$ and $r_B$, both outside the horizon. For a classical static observer at radius $r$, the proper time rate relative to Schwarzschild coordinate time is well defined by $d\tau/dt=\sqrt{1-2M/r}$, and each branch individually has an unambiguous clock rate and redshift.

The toy exists to expose a limitation rather than to make a prediction. Classical general relativity assigns observables along definite worldlines, but it does not specify how to reduce a quantum-superposed observer state $(|r_A\rangle+e^{i\phi}|r_B\rangle)/\sqrt{2}$ to a single geometric readout. Two reasonable but inequivalent prescriptions are contrasted: an incoherent mixture, which averages branch values, and a coherent, phase-sensitive proxy proportional to $|( \text{rate}_A+e^{i\phi}\text{rate}_B)/\sqrt{2}|^2$. Their disagreement, and the explicit dependence on the unobservable phase $\phi$, demonstrates that without an added measurement or decoherence model, the notion of “the” measured metric quantity is operationally undefined.

The JSON output should be read as a structured comparison of branch-defined quantities versus non-unique reductions. The `branch_values` report the well-defined clock rates and redshifts for each radius separately, while `mixture_rate_mean` and `mixture_redshift_mean` show what one obtains after assuming post-decoherence averaging. The `coherent_phase_sensitive_proxy` varies strongly with $\phi$, and its magnitude and sign changes are the key signal of ambiguity; these values should not be over-interpreted as physical clock rates. Instead, they conceptually trace back to combining the same branch rates appearing in $d\tau/dt=\sqrt{1-2M/r}$ under different reduction rules, highlighting that the spread between branches, not the absolute numbers, is what matters in this diagnostic.

---

## Toy 075 — Quantum Clock Time Dilation from Superposed Height

This toy illustrates how gravitational time dilation behaves when a single clock is placed in a quantum superposition of two static radii in a Schwarzschild spacetime. For a static clock at radius $r$ outside the horizon, classical general relativity assigns a well-defined rate relating proper time $\tau$ to Schwarzschild coordinate time $t$ via $d\tau/dt=\sqrt{1-2M/r}$, so that over a coordinate interval $\Delta t$ the elapsed proper time on that branch is $\Delta\tau(r)=\Delta t\sqrt{1-2M/r}$. The setup considers a clock prepared in a superposed state of two heights above a reference radius, leading to two distinct branch values $\Delta\tau_1$ and $\Delta\tau_2$ for the same external spacetime and the same preparation interval.

The toy exists to expose a conceptual limitation rather than to make a physical prediction. In classical GR, specifying a worldline is enough to fix proper time, but a quantum-superposed clock does not follow a single worldline. The code therefore stress-tests the assumption of observer-independent proper time by showing that GR alone supplies only branch-wise answers, while any attempt to compress them into a single elapsed time requires extra postulates such as decoherence, collapse, or an operational readout rule. This is emphasized by comparing a simple incoherent “mixture” assignment $\Delta\tau_{\mathrm{mix}}=(\Delta\tau_1+\Delta\tau_2)/2$ with a deliberately phase-sensitive coherent proxy that depends on the relative phase $\phi$ between branches, demonstrating that different reductions need not agree even in principle.

The JSON output should be read as a diagnostic map of this ambiguity rather than as a prediction of clock behavior. For each sampled pair of heights and phase, the `branch_values` list the classical rates and elapsed times $\Delta\tau_1$ and $\Delta\tau_2$, while `delta_tau_separation_abs` quantifies how far apart those branch times are for the same $\Delta t$. The `mixture_delta_tau_mean` shows one possible coarse-grained assignment, whereas the `coherent_phase_sensitive_proxy` varies strongly with $\phi$, including near-zero values for destructive interference, signaling that no unique scalar “elapsed time” is enforced by the model. These magnitudes should not be over-interpreted as observable times or probabilities; they only connect back to the defining equations by illustrating how branch-wise $\Delta\tau(r)$ values resist reduction to a single, phase-independent number without additional structure beyond classical GR.

---

## Toy 076 — Causal Structure Underdetermined by Entanglement

This toy builds a simple four-subsystem model (nodes A–D) where the only “data” are symmetric pairwise entanglement weights $I_{ij}\ge 0$ on an undirected chain (A–B–C–D) and a distance proxy derived from them. The proxy assigns an edge length $d_{ij}=1/(I_{ij}+\epsilon)$ (with small $\epsilon>0$) on pairs with $I_{ij}>0$, treats non-edges as infinite, and then uses shortest paths to produce all-pairs distances. The quantity of interest is not a physical field but the induced relational structure: what aspects of “spacetime-like” geometry (here, distances) and “causal-like” ordering (here, reachability in a directed acyclic graph) can be inferred from the same entanglement pattern.

The toy exists to stress-test an entanglement-first reconstruction idea: if one tries to read off causal structure from entanglement structure alone, the result can be operationally underdetermined. In this setup, the entanglement graph fixes only an undirected weighted support (and therefore fixes the distance proxy), but it does not select a unique time orientation or causal precedence relation; two inequivalent causal DAGs can both respect the same undirected support. Concretely, “forward_chain” (A→B→C→D) and “reverse_chain” (D→C→B→A) are both compatible with the same $I_{ij}$ and the same shortest-path distances, yet they imply different causal reachability relations, exposing that entanglement-derived geometry does not uniquely pin down causal order.

To interpret the JSON, start with `local_observables.entanglement_weights_I` and `distance_proxy_shortest_paths`: these encode the undirected input (equal nonzero weights on nearest neighbors) and the derived metric-like structure (nearest neighbors at $\approx 1/(3+\epsilon)$, endpoints at roughly three times that, and zeros on the diagonal). Then compare `causal_structure.dag_adjacency` and `causal_structure.reachability` across the two `sample_points`: the key signal is the *change* in reachability (e.g., in the forward option A can reach D, while in the reverse option A cannot reach D), even though the distance matrix is identical in both entries. What should not be over-interpreted is any notion that the DAGs are “predicted” by the distances or that the distances encode an arrow of time; the point is precisely that the same $d_{ij}$ constructed from $I_{ij}$ is compatible with multiple, inequivalent reachability patterns, so magnitudes in the distance matrix should be read only as an undirected proximity proxy tied to $d_{ij}=1/(I_{ij}+\epsilon)$, not as evidence for a unique causal direction.

---

## Toy 077 — Light cone superposition in Minkowski spacetime

This toy illustrates how an otherwise perfectly ordinary causal geometry becomes operationally ambiguous when the “emission event” is not classically localized. In 1+1 dimensional Minkowski spacetime (with $c=1$), a classical source that emits at coordinate time $t=0$ from position $x_0$ determines a definite future light-cone slice at any later time $t>0$, namely $x \in [x_0-t,\,x_0+t]$. Here the setup replaces the single emission point with two possible emission events, A at $x_A$ and B at $x_B$, treated as branches of a spatial superposition at the same coordinate time; the quantity of interest is the set of future positions that can be causally influenced at a given time slice, and whether that set is uniquely defined.

The toy exists to stress-test a classical assumption: “given the source event, the causal future is uniquely defined.” In classical GR/SR, the light cone is tied to a definite apex event; in a superposition of distinct apices, there is no single classical object that plays that role without adding a measurement/reduction rule that selects one branch (or specifies how to combine them). The diagnostic focuses on the branch intervals $I_A(t)$ and $I_B(t)$ and the fact that even the pragmatic alternative—taking the union of branch futures—can change topology as time increases, with the union being disconnected precisely when $|x_A-x_B|>2t$. This is not a prediction about what “really happens,” but a demonstration that classical causal predictability can become non-unique when the localization premise is removed.

To interpret the JSON, treat each `sample_points[i]` as a fixed-time slice $t$ and read `branch_light_cone_slices.A` and `.B` as the two classical intervals $[x_A-t,x_A+t]$ and $[x_B-t,x_B+t]$ at that time. The key trends are in `nonuniqueness`: `disconnected_union` flags whether the union of branch futures is split into two separated intervals (true at $t=0.25,0.5$ here, consistent with `ambiguity_ratio_abs_dx_over_2t` being $>1$), while `union_length` and `overlap_length` track how the combined “reachable set” grows and begins to overlap (overlap becomes nonzero by $t=2.0$ here). What should *not* be over-interpreted is any physical mixing or interference between branches: the `superposed_source_state` phase is intentionally unspecified and no measurement model is implemented, so `uniqueness_status:false` is the point—these numbers are an operational summary of how $I_A(t)$ and $I_B(t)$ relate, not evidence for a single well-defined light cone beyond the classical formula $x \in [x_0-t,x_0+t]$ applied branch-by-branch.

---

## Toy 078 — Relational time depends on the chosen clock factorization

This toy constructs a “timeless” two-qubit quantum state and then recovers an effective notion of “time” by conditioning the system qubit on outcomes of a designated clock qubit. The setup starts from the entangled state $|\Psi\rangle=(|00\rangle+|11\rangle)/\sqrt{2}$ on $H=H_C\otimes H_S$, and defines a “time label” by measuring the clock in the $Z$ basis and post-selecting. For each clock outcome $c\in\{0,1\}$ it reports the conditional system state $\rho_{S|c}=\mathrm{Tr}_C[(P_c\rho P_c)]/p(c)$, where $P_c$ projects the clock onto $|c\rangle$, $\rho=|\Psi\rangle\langle\Psi|$, and $p(c)$ is the post-selection probability.

The point is to stress-test the Page–Wootters-style idea that time can “emerge” from correlations rather than being fundamental, by exposing that the “clock” is not uniquely defined. The toy compares two factorizations: (A) the obvious split where qubit 0 is the clock and qubit 1 is the system, and (B) an alternative split obtained by first applying a fixed global unitary (here a CNOT with qubit 0 as control) and then declaring qubit 0 of the transformed basis to be the clock. Because $\rho_{S|c}$ depends on the projector $P_c$ and on what is meant by $\mathrm{Tr}_C$ (i.e., which degrees of freedom are treated as “the clock”), the same global state can yield different relational “time slices,” illustrating an underdetermination rather than a physical prediction.

To read the JSON, focus on the conditional system density matrices under `sample_points[*].local_observables.conditional_system_states` and the summary trace distances under `observables.summary`. In factorization A, the two time labels yield orthogonal pure states (purity 1 and a within-factorization trace distance of 1.0), while in factorization B both labels produce the same conditional system state (within-factorization distance 0.0), so the “clock” no longer distinguishes slices. The cross-factorization distances show that label 0 can agree (distance 0.0) while label 1 disagrees maximally (distance 1.0), so what matters is whether distances are near 0 (robust across clock choices) or near 1 (clock-choice dependent). What should not be over-interpreted is any notion of real dynamics or causal structure: these labels are bookkeeping for conditioning, and the reported values are diagnostics of how $\rho_{S|c}$ changes with the choice of factorization rather than evidence for a unique physical time parameter.

---

## Toy 079 — Thermal time from modular flow becomes state-dependent out of equilibrium

This toy takes a three-level quantum system with Hamiltonian $H=\mathrm{diag}(0,E,2E)$ and asks whether a notion of “time flow” can be extracted from the thermodynamic state itself. The state is a density matrix $\rho$, and the toy constructs the modular Hamiltonian $K=-\ln\rho$ (on the support of $\rho$, with tiny eigenvalues clipped) as the generator of modular flow. In equilibrium Gibbs cases, $\rho\propto e^{-\beta H}$ makes $K$ proportional to $H$ up to an additive constant, so the modular flow can be interpreted as physical time evolution with a simple rescaling set by $\beta$.

The toy exists to stress-test the thermal time hypothesis by exposing where “time from state” stops being operationally unique rather than making any claim about real dynamics. The conceptual failure mode is that away from equilibrium there is generally no single inverse temperature $\beta$ that makes $K$ line up with $H$, and with coherence one can even get $[K,H]\neq 0$, so the modular generator defines a different flow than the Hamiltonian does. Concretely, the toy probes whether one can meaningfully regard $K$ as $K\approx \beta H + cI$ (with $c$ an arbitrary constant offset) as a uniqueness criterion; when this approximation fails, the mapping between modular time and physical time becomes state-dependent and ambiguous.

To read the JSON, focus on each `sample_points[i].local_observables` block: `beta_fit` and `c_fit` come from a least-squares fit of the diagonal of $K$ to $\beta H + cI$ in the energy basis, `fit_residual_rms` quantifies how well that diagonal proportionality holds, and `commutator_fro_norm` measures misalignment via $\|[K,H]\|_F$. In the equilibrium case, `beta_fit≈1`, `fit_residual_rms≈0`, and `commutator_fro_norm=0`, indicating $K$ tracks $H$ cleanly (so `thermal_time_defined_as_unique_scaling=true`), whereas the non-equilibrium diagonal state keeps `commutator_fro_norm=0` but has a large residual (so it cannot be captured by one $\beta$), and the coherent state shows both a nonzero residual and a sizable commutator norm (so modular flow is not even generated by something commuting with $H$). What should not be over-interpreted are the fitted numbers as “the temperature” or the commutator norm as a dynamical rate; they are diagnostics of how the computed $K=-\ln\rho$ relates to the fixed $H$ and whether the JSON is telling you “modular time can be consistently identified with Hamiltonian time” or “the identification is state-dependent and breaks in a controlled way.”

---

## Toy 080 — Near-horizon observer disagreement via blue-shifted Hawking thermality

This toy compares what two different “observers” would claim about local physics just outside a Schwarzschild horizon. It sets a black hole mass $M$ and samples radii $r=2M(1+\epsilon)$ with $\epsilon>0$, using the Hawking temperature at infinity $T_H=1/(8\pi M)$ as the baseline. A static (hovering) observer at radius $r$ is assigned a blue-shifted local temperature $T_{\text{loc}}(r)=T_H/\sqrt{f(r)}$ where $f(r)=1-2M/r$, and that temperature is then converted into a proxy local energy density via a thermal formula; the quantity of interest is the resulting “static-observer energy density proxy” near the horizon, contrasted against an infalling-vacuum proxy of zero.

The toy exists to stress-test complementarity-style claims by making an observer-dependence clash numerically explicit, not to predict what a real detector will see. In semiclassical language, the Tolman blue shift drives $T_{\text{loc}}$ upward as $f(r)\to 0^+$, while the equivalence-principle expectation for a freely falling observer is “approximately vacuum at the horizon,” modeled here as $\rho_{\text{infall}}\approx 0$; the toy then exposes how rapidly a static-observer thermal proxy $\rho_{\text{th}}(T)=(\pi^2/30)T^4$ can become large as $\epsilon\to 0$. Interpreting that as a “firewall proxy” illustrates the conceptual failure mode: you cannot naively keep unitarity, locality, and equivalence all at once while insisting both descriptions represent the same local state, so the output should be read as a limitation/stress test of a semiclassical observer+state package rather than as a resolution.

To read the JSON, focus on how the blue-shift factor and thermal proxy amplify as you move closer to the horizon: for each sample point, `local_observables.f_r` encodes $f(r)$, `local_temperature_static_T_loc` encodes $T_{\text{loc}}(r)$, and `energy_density_proxy_static_rho_th` encodes $\rho_{\text{th}}(T_{\text{loc}})$. The key trend is monotonic growth as $\epsilon$ decreases: in the provided run with $M=10$, $T_H\approx 3.98\times 10^{-3}$ while `T_loc` rises from $\sim 1.32\times 10^{-2}$ at $\epsilon=0.1$ to $\sim 3.98\times 10^{-1}$ at $\epsilon=10^{-4}$, and the corresponding `rho_th` increases from $\sim 1.0\times 10^{-8}$ to $\sim 8.2\times 10^{-3}$; `causal_structure.energy_density_difference` is the same number because the infalling proxy is fixed to zero, and `contradiction_detected` flags that mismatch. What should not be over-interpreted is the absolute magnitude or the “always true” contradiction flag as a claim about actual horizon drama: the thermal mapping $\rho_{\text{th}}(T)$ is a stand-in for “high-energy excitations,” not an AMPS derivation, and the curvature invariants being modest in `curvature_invariants` is a reminder that the toy’s tension is about the quantum state/entanglement and observer description, conceptually linking the reported JSON values back to the anchors $T_{\text{loc}}(r)=T_H/\sqrt{f(r)}$ and $\rho_{\text{th}}(T)=(\pi^2/30)T^4$ rather than to any physical prediction.

---

